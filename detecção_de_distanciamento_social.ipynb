{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7ZHdWPOMG2rsOaw76Q1Ar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dedemasutti/AI-projects/blob/master/detec%C3%A7%C3%A3o_de_distanciamento_social.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CONFIGURANDO OS VALORES DAS VARIAVÉIS"
      ],
      "metadata": {
        "id": "5BHYfDmeF4oB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0LosxVt6EKUF"
      },
      "outputs": [],
      "source": [
        "MIN_CONFIDENCE = 0.5\n",
        "MIN_THRESHOLD = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE A DISTANCIA MÍNIMA SEGURA EM PIXELS(two or more peoples)\n",
        "MIN_DISTANCE = 50"
      ],
      "metadata": {
        "id": "8gqPrxbOEkzY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CROWD DETECTOR "
      ],
      "metadata": {
        "id": "U7vxQ6fwFpMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "UjfSm-z3Ekop"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crowd_detector(frame, net, ln, personIdx = 0):\n",
        "  #pega as dimensões do frame e inicia lista de resultados\n",
        "  (H,W) = frame.shape[:2]\n",
        "  results = []\n",
        "\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1/255.0(416,416, swapRB = True, crop =False))\n",
        "  net.setInput(blob)\n",
        "  layerOutputs = net.forward(ln)\n",
        "\n",
        "  boxes = []\n",
        "  centroids = []\n",
        "  confidences = []\n",
        "\n",
        "# loop over each of the layers output\n",
        "  for output in layerOutputs:\n",
        "\n",
        "# loop over each of the detections\n",
        "    for detection in output:\n",
        "      scores = detection[5:]\n",
        "      classID = np.argmax(scores)\n",
        "      confidence = scores[classID]\n",
        "\n",
        "      if classID == personIdx and confidence > MIN_CONFIDENCE:\n",
        "        box = detection[0:4 * np.array[W,H,W,H]\n",
        "        (centerX, centerY, width, height = box.astype('int')\n",
        "\n",
        "\n",
        "        x = int(centerX - (width / 2))\n",
        "        y = int(centerY -(height / 2))\n",
        "      \n",
        "        boxes.append([x, y, int(width), int(height)])\n",
        "        centroids.append((centerX, centerY))\n",
        "        confidences.append(float(confidence))\n",
        "\n",
        "\n",
        "idxs = cv2.dnn.NMSBoxes(boxes,confidences,MIN_CONFIDENCE,MIN_THRESHOLD)\n",
        "\n",
        "if len(idxs) > 0:\n",
        "  for i in idxs.flatten():\n",
        "    (x, y) = (boxes[i][0], boxes[i][1])\n",
        "    (w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "    r = (confidences[i], (x, y, x + w, y + h), centroids[i]\n",
        "    results.append(r)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "TGyLf0IbGSEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GRAB FRAMES FROM VIDEO AND MAKE PREDICTION MEASURING DISTANCES OF DETECTED PEOPLE "
      ],
      "metadata": {
        "id": "pVV1w9CdWoL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports \n",
        "from google.colab.patches import cv2_imshow\n",
        "from spicy.spatial import distance as dist\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import cv2\n",
        "import imutils\n"
      ],
      "metadata": {
        "id": "oYl77cGbW37x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construct the argument parse and parse the arguments\n",
        "from numpy.core.fromnumeric import shape\n",
        "from traitlets.traitlets import default\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--input\",type = str, default=\"\",\n",
        "    help = \"path to (optional)input video file\")\n",
        "ap.add_argument(\"-0\", \"--output\", type = str, default = \"\",\n",
        "    help = \"path to(optional) output video file\")\n",
        "ap.add_argument(\"-d\", \"--display\", type = str, default = \"\",\n",
        "    help = \"whether or not output frame should be displayed\")\n",
        "args = vars(ap.parse_args([\"--input\",\"file path\", \"--output\",\"my_output.avi\",\"--display\",\"1\"]))\n",
        "\n",
        "# Load the COCO class labels\n",
        "labelsPath = os.path.sep.join([\"file path\"])\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\\\")\n",
        "\n",
        "#derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = os.path.sep.join([\"file yolo.weights path\"])\n",
        "configPath = os.path.sep.join([\"file yolo.config path\"])\n",
        "\n",
        "# Load our YOLO object detector trained on Dataset\n",
        "print(\"[INFO] loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(weightsPath, configPath)\n",
        "\n",
        "# Determine only the output layer names that we need from YOLO\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i[0] -1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "#Initialize the video stream and pointer to output video file\n",
        "print(\"[INFO] accessing video stream...\")\n",
        "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
        "writer = None\n",
        "\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "# read the next frame from the file\n",
        "  (grabbed, frame) = vs.read()\n",
        "# if the frame was not grabbed ,then we have reached the end of the stream\n",
        "  if not grabbed:\n",
        "    break\n",
        "\n",
        "# resize the frame and then detect people\n",
        "frame = imutils.resize(frame, width = 500)\n",
        "results = detect_people(frame, net, ln, personIdx=LABELS.index(\"person\"))\n",
        "\n",
        "# Initialize the set of indexes that violate minimum social distance\n",
        "violate = set()\n",
        "\n",
        "# ensure there are at least two people \n",
        "#detected(required in order to compute our pairwise distance maps)\n",
        "if len(results) >= 2:\n",
        "  #extract all centroids from the results and \n",
        "  #compute the euclidean distance between all pairs of the centroids\n",
        "  centroids = np.array([r[2] for r in results])\n",
        "  d = dist.cdist(centroids, centroids, metric = \"euclidean\")\n",
        "  #loop over the upper triangular distance matrix\n",
        "  for i in range(0, d.shape[0]):\n",
        "    for j in range(i + 1, d.shape[1]):\n",
        "      #check if the distance between any two centroid\n",
        "      #pairs are less than the configured number of pixels\n",
        "      if d[i,j] < MIN_DISTANCE:\n",
        "        #update our violation set with the indexes of centroid pairs\n",
        "        violate.add(i)\n",
        "        violate.add(j)\n",
        "\n",
        "# loop over the results\n",
        "for (i(prob,bbox, centroid)) in enumerate(results):\n",
        "  # extract bounding box and centroid coordinates then initialize \n",
        "  # the color of annotation\n",
        "  (startX, startY, endX, endY) = bbox\n",
        "  (cX, cY) = centroid\n",
        "  color = (0,255,0)\n",
        "  # if the index pair exists whithin the violation set\n",
        "  # update the color\n",
        "  if i in violate:\n",
        "    color = (0,0,255)\n",
        "\n",
        "#draw(1) a bounding box around person and (2)\n",
        "#the centroid coordinates of the person.\n",
        "cv2.rectangle(frame(startX, startY),(endX, endY),color, 2)\n",
        "cv2.circle(frame)\n",
        "\n",
        "#draw the total number of social distance violation\n",
        "#on the output frame\n",
        "text = \"alerta para distanciamento social\": {}.format(len(violate))\n",
        "cv2.putText(frame, text,(10,frame.shape[0] - 25),\n",
        "            cv2.FONT_HERSHEY_DUPLEX, 0.75,(0,0,255),3)\n",
        "\n",
        "# check to see if the output frame should be displayed to our screen\n",
        "if args[\"display\"] > 0:\n",
        "  #show the output frame\n",
        "  cv2.imshow(frame)\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "  # 'q' key should pressed to break the loop\n",
        "  if key == ord(\"q\"):\n",
        "    break\n",
        "  #if an output video file path has been supplied and the\n",
        "  # videos writer has not been initialized. \n",
        "  if args[\"output\"] != \"\" and writer is None:\n",
        "    #initialize our video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "    writer = cv2.VideoWriter(args[\"output\"], fourcc, 25, frame.shape[1],\n",
        "                             frame.shape[0], True)\n",
        "    #if the video writer is not None write the frame to the output video file\n",
        "    if writer is not None:\n",
        "      writer.write(frame)"
      ],
      "metadata": {
        "id": "ZqkI27zYonr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}